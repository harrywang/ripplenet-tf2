{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(555)\n",
    "DATASET = 'movie'  # or 'book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news dataset has not been included yet\n",
    "RATING_FILE_NAME = dict({'movie': 'movie_ratings.dat', 'book': 'book_ratings.csv', 'news': 'ratings.txt'})\n",
    "\n",
    "# different rating files have different separators\n",
    "SEP = dict({'movie': '::', 'book': ';', 'news': '\\t'})\n",
    "\n",
    "THRESHOLD = dict({'movie': 4, 'book': 0, 'news': 0})\n",
    "\n",
    "# whether to skip the heading line in file\n",
    "SKIP_LINE = dict({'movie': 0, 'book': 1, 'news': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id2index = dict()\n",
    "item_index_old2new = dict()\n",
    "relation_id2index = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_index2entity_id_rehashed.txt maps the id in movie and book dataset\n",
    "# to the kg satori id\n",
    "def read_item_index_to_entity_id_file():\n",
    "    file = 'data/' + DATASET + '/item_index2entity_id_rehashed.txt'\n",
    "    print('reading item index to entity id file: ' + file + ' ...')\n",
    "    i = 0\n",
    "    for line in open(file, encoding='utf-8').readlines():\n",
    "        item_index = line.strip().split('\\t')[0]  # item id from the movie dataset\n",
    "        satori_id = line.strip().split('\\t')[1]  # satori id from the kg\n",
    "        item_index_old2new[item_index] = i\n",
    "        entity_id2index[satori_id] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "reading item index to entity id file: data/movie/item_index2entity_id_rehashed.txt ...\n"
    }
   ],
   "source": [
    "read_item_index_to_entity_id_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('0', 0),\n ('1', 1),\n ('2', 2),\n ('3', 3),\n ('4', 4),\n ('5', 5),\n ('6', 6),\n ('7', 7),\n ('8', 8),\n ('9', 9),\n ('10', 10),\n ('11', 11),\n ('12', 12),\n ('13', 13),\n ('14', 14),\n ('15', 15),\n ('16', 16),\n ('17', 17),\n ('18', 18),\n ('19', 19)]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "list(entity_id2index.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('1', 0),\n ('2', 1),\n ('3', 2),\n ('4', 3),\n ('5', 4),\n ('8', 5),\n ('10', 6),\n ('11', 7),\n ('12', 8),\n ('13', 9),\n ('14', 10),\n ('15', 11),\n ('17', 12),\n ('18', 13),\n ('19', 14),\n ('20', 15),\n ('21', 16),\n ('25', 17),\n ('27', 18),\n ('29', 19)]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "list(item_index_old2new.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2445"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "## these are the items found in the kg (such as movie and book)\n",
    "item_set = set(item_index_old2new.values())\n",
    "len(item_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rating():\n",
    "    file = 'data/' + DATASET + '/' + RATING_FILE_NAME[DATASET]\n",
    "    skip_line = SKIP_LINE[DATASET] - 1  # skip heading if needed\n",
    "\n",
    "    print('reading rating file ...')\n",
    "    item_set = set(item_index_old2new.values())  # len(item_set) is 2445 the total number of items\n",
    "\n",
    "    # change scaled ratings to binary: positive or negative\n",
    "    user_pos_ratings = dict()  \n",
    "    user_neg_ratings = dict()\n",
    "\n",
    "    # open the rating file\n",
    "    for i, line in enumerate(open(file, encoding='utf-8').readlines()):\n",
    "        if i == skip_line:  # skip heading if needed\n",
    "            continue\n",
    "        array = line.strip().split(SEP[DATASET])  # different dataset has different separators \n",
    "\n",
    "        # remove prefix and suffix quotation marks for book rating dataset\n",
    "        if DATASET == 'book':\n",
    "            array = list(map(lambda x: x[1:-1], array))\n",
    "        \n",
    "        # array user-SEP-movie-SEP-rating\n",
    "\n",
    "        item_index_old = array[1] # old index in the original rating file\n",
    "\n",
    "        # the following logic is that we only use the movie that showed up in the kg\n",
    "        # the item is not in the final item set\n",
    "        if item_index_old not in item_index_old2new:\n",
    "            continue  # skip the one that are not in the kg\n",
    "        #print(f'{item_index_old} in kg')\n",
    "        item_index = item_index_old2new[item_index_old]\n",
    "        #print(f'{item_index} is the new index in kg')\n",
    "\n",
    "        user_index_old = int(array[0])\n",
    "\n",
    "        rating = float(array[2])  # convert to pos and neg ratings\n",
    "        if rating >= THRESHOLD[DATASET]:\n",
    "            if user_index_old not in user_pos_ratings:\n",
    "                user_pos_ratings[user_index_old] = set()\n",
    "            user_pos_ratings[user_index_old].add(item_index)\n",
    "        else:\n",
    "            if user_index_old not in user_neg_ratings:\n",
    "                user_neg_ratings[user_index_old] = set()\n",
    "            user_neg_ratings[user_index_old].add(item_index)\n",
    "\n",
    "    #print(user_pos_ratings)\n",
    "    print('converting rating file ...')\n",
    "    writer = open('data/' + DATASET + '/ratings_final.txt', 'w', encoding='utf-8')\n",
    "    user_cnt = 0\n",
    "    user_index_old2new = dict()\n",
    "    for user_index_old, pos_item_set in user_pos_ratings.items():\n",
    "        if user_index_old not in user_index_old2new:\n",
    "            user_index_old2new[user_index_old] = user_cnt\n",
    "            user_cnt += 1\n",
    "        user_index = user_index_old2new[user_index_old]\n",
    "\n",
    "        for item in pos_item_set:\n",
    "            writer.write('%d\\t%d\\t1\\n' % (user_index, item)) # user_index tab item tab 1\n",
    "        unwatched_set = item_set - pos_item_set\n",
    "        # the following part logic is not quite clear\n",
    "        # see this issue: https://github.com/hwwang55/RippleNet/issues/18\n",
    "        # basically, the author's logic is if a movie rating is >=4 then, it's positive\n",
    "        # otherwise, remove the negative items from the item set if any and then\n",
    "        # random choose same amount of items as the positive ones as the negative ones\n",
    "        # note here 1 means interested 0 means not interested - low rating is also considered interestd by authors\n",
    "        if user_index_old in user_neg_ratings:\n",
    "            unwatched_set -= user_neg_ratings[user_index_old] # remove the negative ones\n",
    "        for item in np.random.choice(list(unwatched_set), size=len(pos_item_set), replace=False):  # random choose from the remaining set\n",
    "            writer.write('%d\\t%d\\t0\\n' % (user_index, item))\n",
    "    writer.close()\n",
    "    print('number of users: %d' % user_cnt)\n",
    "    print('number of items: %d' % len(item_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "reading rating file ...\nconverting rating file ...\nnumber of users: 6036\nnumber of items: 2445\n"
    }
   ],
   "source": [
    "convert_rating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_kg():\n",
    "    print('converting kg file ...')\n",
    "    entity_cnt = len(entity_id2index)\n",
    "    relation_cnt = 0\n",
    "\n",
    "    writer = open('data/' + DATASET + '/kg_final.txt', 'w', encoding='utf-8')\n",
    "\n",
    "    files = []\n",
    "    if DATASET == 'movie':\n",
    "        files.append(open('data/' + DATASET + '/kg_part1_rehashed.txt', encoding='utf-8'))\n",
    "        files.append(open('data/' + DATASET + '/kg_part2_rehashed.txt', encoding='utf-8'))\n",
    "    else:\n",
    "        files.append(open('data/' + DATASET + '/kg_rehashed.txt', encoding='utf-8'))\n",
    "\n",
    "    for file in files:  # each line is a triplet head-TAB-relation-TAB-tail: 2451\tfilm.actor.film\t2452\n",
    "        for line in file:\n",
    "            array = line.strip().split('\\t')  # split by tab\n",
    "            head_old = array[0]  # head\n",
    "            relation_old = array[1]  # relation\n",
    "            tail_old = array[2]  # tail\n",
    "\n",
    "            if head_old not in entity_id2index:\n",
    "                entity_id2index[head_old] = entity_cnt\n",
    "                entity_cnt += 1\n",
    "            head = entity_id2index[head_old]\n",
    "\n",
    "            if tail_old not in entity_id2index:\n",
    "                entity_id2index[tail_old] = entity_cnt\n",
    "                entity_cnt += 1\n",
    "            tail = entity_id2index[tail_old]\n",
    "\n",
    "            if relation_old not in relation_id2index:\n",
    "                relation_id2index[relation_old] = relation_cnt\n",
    "                relation_cnt += 1\n",
    "            relation = relation_id2index[relation_old]\n",
    "\n",
    "            writer.write('%d\\t%d\\t%d\\n' % (head, relation, tail))\n",
    "\n",
    "    writer.close()  # the kg final: head-TAB-relationID-TAB-tail\n",
    "    print('number of entities (containing items): %d' % entity_cnt)\n",
    "    print('number of relations: %d' % relation_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "converting kg file ...\nnumber of entities (containing items): 182011\nnumber of relations: 12\ndone\n"
    }
   ],
   "source": [
    "convert_kg()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitvenvvenv6c45b8dbf8fb4994a418e50f006017cc",
   "display_name": "Python 3.7.6 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}